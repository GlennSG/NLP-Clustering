{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 100 science fiction novels from project gutenberg. \n",
    "\n",
    "Source: https://www.gutenberg.org/wiki/Science_Fiction_(Bookshelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import books_files as books \n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def text_cleaner(text_data):\n",
    "    text = re.sub(\"[^a-zA-Z]\",' ',text_data)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    lmz = WordNetLemmatizer()\n",
    "    text = [lmz.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "book_text = []\n",
    "authors = []\n",
    "\n",
    "for k,v in books.url_dict.items():\n",
    "    for url in v:\n",
    "        response = request.urlopen(url)\n",
    "        raw = response.read().decode('utf8')\n",
    "        s = StringIO(raw)\n",
    "        text_lines = []\n",
    "        for i,line in enumerate(s):\n",
    "            text_lines.append(line)\n",
    "            if \"START OF THIS PROJECT\" in line:\n",
    "                start_index = (i+1)\n",
    "            if \"END OF THIS PROJECT\" in line:\n",
    "                end_index = i\n",
    "        book_text.append(' '.join(text_lines[start_index:end_index]))\n",
    "        authors.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# 100 different books\n",
    "print(len(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# 16 different authors\n",
    "print(len(set(authors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>Author_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\r\\n \\r\\n \\r\\n \\r\\n Produced by Greg Weeks, Ma...</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r\\n \\r\\n \\r\\n \\r\\n Produced by Sankar Viswana...</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\n \\r\\n \\r\\n \\r\\n Produced by Sankar Viswana...</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    Author  Author_ID\n",
       "0  \\r\\n \\r\\n \\r\\n \\r\\n Produced by Greg Weeks, Ma...  Anderson          0\n",
       "1  \\r\\n \\r\\n \\r\\n \\r\\n Produced by Sankar Viswana...  Anderson          0\n",
       "2  \\r\\n \\r\\n \\r\\n \\r\\n Produced by Sankar Viswana...  Anderson          0"
      ]
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame()\n",
    "text_df[\"Text\"] = book_text\n",
    "text_df[\"Author\"] = authors\n",
    "text_df[\"Author_ID\"] = text_df[\"Author\"].factorize()[0]\n",
    "text_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bag of words\n",
    "w2v = []\n",
    "corpus = []\n",
    "for text in book_text:\n",
    "    clean_text = text_cleaner(text)\n",
    "    corpus.append(clean_text)\n",
    "    clean_text = nltk.word_tokenize(clean_text)\n",
    "    w2v.append(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_v = TfidfVectorizer(max_df=0.5,\n",
    "                             min_df =2,\n",
    "                             stop_words=\"english\",\n",
    "                             lowercase=False,\n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',\n",
    "                             smooth_idf=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = tfidf_v.fit_transform(corpus)\n",
    "y = text_df.iloc[:,2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = tfidf_v.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to use cosine similarity to generate a measure of similarity between each book in the corpus (tf-idf matrix). We are subtracting the cosine similarity from 1 to get the cosine distance which will be used later on to plot the clusters on a euclidean 2D plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "n_clusters = range(1,10)\n",
    "for i in n_clusters:\n",
    "    kmeans = KMeans(n_clusters=i,random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (23,13)\n",
    "plt.plot(n_clusters,wcss)\n",
    "plt.title(\"The Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.xticks(n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 6 clusters seems optimal\n",
    "num_clusters = 6\n",
    "km = KMeans(n_clusters=num_clusters,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "km.fit(X_train)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "books = {\"author\":y_train,\"book_text\":X_train,\"cluster\":clusters}\n",
    "frame = pd.DataFrame(books,index=[clusters],columns=[\"author\",\"book_text\",\"cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "author_id = {}\n",
    "for author in list(text_df[\"Author_ID\"]):\n",
    "    if author not in author_id:\n",
    "        author_id[author] = text_df.loc[text_df[\"Author_ID\"]==author,\"Author\"].iloc[0]\n",
    "author_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting top 10 words and authors per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Authors per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "cluster_names = {}\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_names[i] = \"\"\n",
    "    print(\"Cluster %d:\"%i)\n",
    "    print(\"Top 10 words:\")\n",
    "    for ind in order_centroids[i,:10]:\n",
    "        cluster_names[i] += (\"{} \".format(str(words[ind])))\n",
    "        print(\"%s\"%words[ind])\n",
    "    cluster_names[i] = ','.join(cluster_names[i].split())\n",
    "    print()\n",
    "    print(\"Cluster %d authors:\" % i, end='')\n",
    "    for author in frame.loc[i]['author'].values.tolist():\n",
    "        print(' %s,' % author_id[author], end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clusters visual (dimension reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_norm = normalize(dist)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variance_explained = pca.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components: \",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xs,ys = X_pca[:,0],X_pca[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#ffe059', 1: '#9cff07', 2: '#ff0707', 3: '#07ffc1', 4: '#a407ff',5: '#ff07bc'}\n",
    "\n",
    "#cluster_names = {0: 'joe, kenzie, guy, instructor, chessman, chief, watson, brett, buckner, baron', \n",
    "                 #1: 'johnny, mead, kennedy, captain, geoffrey, dane, commodore, ron, aunt, wilkins', \n",
    "                 #2: 'nipe, stanton, mannheim, bart, yoritomo, martin, colonel, farnsworth, rat, wang', \n",
    "                 #3: 'colonel, guesser, zen, porter, senator, gibson, bending, charlie, parker, stoker', \n",
    "                 #4: 'gordon, thompson, ross, muller, mar, doctor, henry, fisher, doc, reactor',\n",
    "                 #5: 'martian, robot, robert, kitty, presently, miller, animal, redwood, beast, blackie'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, author=[author_id[i] for i in y_train])) \n",
    "\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(21, 10))\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='False',      # ticks along the bottom edge are off\n",
    "        top='False',         # ticks along the top edge are off\n",
    "        labelbottom='False')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='False',      # ticks along the bottom edge are off\n",
    "        top='False',         # ticks along the top edge are off\n",
    "        labelleft='False')\n",
    "\n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['author'], size=8)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Cluster visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 25)) # set size\n",
    "\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=[author_id[i] for i in y_train]);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='False',      # ticks along the bottom edge are off\n",
    "    top='False',         # ticks along the top edge are off\n",
    "    labelbottom='False')\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.tight_layout() #show plot with tight layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying book texts by author (Supervised Learning Application)\n",
    "\n",
    "We will attempt to use supervised and unsupervised learning techniques to identify the authors by their respective book texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use PCA to generate features (features should be less than 100, only have 100 different books)\n",
    "#X_norm_features = normalize(X.toarray())\n",
    "pca = PCA(0.95)\n",
    "X_features = pca.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypred_km = km.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_rough = np.concatenate((X_features,ypred_km[:,None]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(features_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(features_df[90])\n",
    "dummy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df = pd.concat([features_df,dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df = features_df.drop([90.0,5.0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = features_df.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models=[\n",
    "    LogisticRegression(random_state=42),\n",
    "    RandomForestClassifier(n_estimators=300,max_depth=3,random_state=42),\n",
    "    SVC(),\n",
    "    #MultinomialNB(),\n",
    "    XGBClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CV = 6\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model,X,y,scoring=\"accuracy\",cv=CV)\n",
    "    \n",
    "    for fold_idx,accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name,fold_idx,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(entries,columns=[\"model_name\",\"fold_idx\",\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classification Model Performance (XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgc = XGBClassifier()\n",
    "\n",
    "xgc.fit(X_train,y_train)\n",
    "ypred_xgc = xgc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_test,ypred_xgc)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "sns.heatmap(conf_mat,annot=True,fmt='d',xticklabels=[v for k,v in author_id.items()],yticklabels=[v for k,v in author_id.items()])\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# classification report for each class (XGC)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test,ypred_xgc,target_names=[v for k,v in author_id.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "1. Get a benchmark of all supervised learning models (fitting and comparing accuracy scores)\n",
    "\n",
    "2. Evaluate the best model (most accurate, best score) using confusion matrix\n",
    "\n",
    "3. Explore sklearn's classification_report module\n",
    "\n",
    "4. Use word2vec for feature engineering, visualize & interpret clusters made by word2vec (TSNE & Affinity Propagation)\n",
    "\n",
    "5. Extra: Apply supervised learning models to word2vec features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
